{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wicked-bundle",
   "metadata": {},
   "source": [
    "# Image Caption Generation Using Convolutional Neural Network's Xception Model & Long Short Term Memory\n",
    "![alt text](WC.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-rendering",
   "metadata": {},
   "source": [
    "## You saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? \n",
    "![alt text](How.png \"How?\")\n",
    "## With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image. This is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-estonia",
   "metadata": {},
   "source": [
    "# What is Image Caption Generator?\n",
    "## Image caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English.\n",
    "![alt text](test.png \"Caption Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-satellite",
   "metadata": {},
   "source": [
    "## The objective of our project is to learn the concepts of a CNN and LSTM model and build a working model of Image caption generator by implementing CNN with LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-reply",
   "metadata": {},
   "source": [
    "## In this Python project, we will be implementing the caption generator using CNN (Convolutional Neural Networks) and LSTM (Long short term memory). The image features will be extracted from Xception which is a CNN model trained on the imagenet dataset and then we feed the features into the LSTM model which will be responsible for generating the image captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-killer",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "## For the image caption generator, we will be using the Flickr_8K dataset. There are also other big datasets like Flickr_30K and MSCOCO dataset but it can take weeks just to train the network so we will be using a small Flickr8k dataset. The advantage of a huge dataset is that we can build better models.\n",
    "## The Flickr_8k_text folder contains file Flickr8k.token which is the main file of our dataset that contains image name and their respective captions separated by newline(“\\n”).\n",
    "![alt text](dataset.png \"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-serbia",
   "metadata": {},
   "source": [
    "# What is Convolutional Neural Network \n",
    "## Convolutional Neural networks are specialized deep neural networks which can process the data that has input shape like a 2D matrix. Images are easily represented as a 2D matrix and CNN is very useful in working with images. CNN is basically used for image classifications and identifying if an image is a bird, a plane or Superman, etc.\n",
    "![alt text](cnn.jpeg \"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-solomon",
   "metadata": {},
   "source": [
    "## It scans images from left to right and top to bottom to pull out important features from the image and combines the feature to classify images. It can handle the images that have been translated, rotated, scaled and changes in perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-reasoning",
   "metadata": {},
   "source": [
    "# What is Long Short Term Memory \n",
    "## LSTM stands for Long short term memory, they are a type of RNN (recurrent neural network) which is well suited for sequence prediction problems. Based on the previous text, we can predict what the next word will be. It has proven itself effective from the traditional RNN by overcoming the limitations of RNN which had short term memory. LSTM can carry out relevant information throughout the processing of inputs and with a forget gate, it discards non-relevant information.\n",
    "![alt text](lstm.png \"Lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-prison",
   "metadata": {},
   "source": [
    "# Image Caption Generation Model \n",
    "## So, to make our image caption generator model, we will be merging these architectures. It is also called a CNN-RNN model.\n",
    "## CNN is used for extracting features from the image. We will use the pre-trained model Xception.\n",
    "## LSTM will use the information from CNN to help generate a description of the image.\n",
    "![alt text](model_note.png \"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-insider",
   "metadata": {},
   "source": [
    "# First, we import all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "capable-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forbidden-avenue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yashi\\\\Desktop\\\\project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-smile",
   "metadata": {},
   "source": [
    "# We will define 5 functions: \n",
    "## 1.) load_doc( filename ) – For loading the document file and reading the contents inside the file into a string.\n",
    "## 2.) all_img_captions( filename ) – This function will create a descriptions dictionary that maps images with a list of 5 captions.\n",
    "## 3.) cleaning_text( descriptions) – This function takes all descriptions and performs data cleaning. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers. So, a caption like “A man riding on a three-wheeled wheelchair” will be transformed into “man riding on three wheeled wheelchair”\n",
    "## 4.) text_vocabulary( descriptions ) – This is a simple function that will separate all the unique words and create the vocabulary from all the descriptions.\n",
    "## 5.) save_descriptions( descriptions, filename ) – This function will create a list of all the descriptions that have been preprocessed and store them into a file. We will create a descriptions.txt file to store all the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subsequent-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a text file into memory\n",
    "def load_doc(filename):\n",
    "    # Opening the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exempt-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all imgs with their captions\n",
    "def all_img_captions(filename):\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions ={}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "individual-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning- lower casing, removing puntuations and words containing numbers\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for img,caps in captions.items():\n",
    "        for i,img_caption in enumerate(caps):\n",
    "\n",
    "            img_caption.replace(\"-\",\" \")\n",
    "            desc = img_caption.split()\n",
    "\n",
    "            #converts to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            #remove hanging 's and a \n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            #convert back to string\n",
    "\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i]= img_caption\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpha-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vocabulary(descriptions):\n",
    "    # build vocabulary of all unique words\n",
    "    vocab = set()\n",
    "    \n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fitted-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All descriptions in one file \n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc )\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "piano-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text = \"Flickr8k_text\"\n",
    "dataset_images = \"Flickr8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fossil-rebate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of descriptions = 8092\n",
      "Length of vocabulary =  8763\n"
     ]
    }
   ],
   "source": [
    "#we prepare our text data\n",
    "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "#loading the file that contains all data\n",
    "#mapping them into descriptions dictionary img to 5 captions\n",
    "descriptions = all_img_captions(filename)\n",
    "print(\"Length of descriptions =\" ,len(descriptions))\n",
    "\n",
    "#cleaning the descriptions\n",
    "clean_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "#building vocabulary \n",
    "vocabulary = text_vocabulary(clean_descriptions)\n",
    "print(\"Length of vocabulary = \", len(vocabulary))\n",
    "\n",
    "#saving each description to file \n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-responsibility",
   "metadata": {},
   "source": [
    "# Extracting the feature vector from all images \n",
    "## This technique is also called transfer learning, we don’t have to do everything on our own, we use the pre-trained model that have been already trained on large datasets and extract the features from these models and use them for our tasks. We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. We can directly import this model from the keras.applications. Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector. \n",
    "## model = Xception( include_top=False, pooling=’avg’ )\n",
    "## The function extract_features() will extract features for all images and we will map image names with their respective feature array. Then we will dump the features dictionary into a “features.p” pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "helpful-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "        model = Xception( include_top=False, pooling='avg' )\n",
    "        features = {}\n",
    "        for img in tqdm(os.listdir(directory), position=0):\n",
    "            filename = directory + \"/\" + img\n",
    "            image = Image.open(filename)\n",
    "            image = image.resize((299,299))\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #image = preprocess_input(image)\n",
    "            image = image/127.5\n",
    "            image = image - 1.0\n",
    "            \n",
    "            feature = model.predict(image)\n",
    "            features[img] = feature\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2048 feature vector\n",
    "features = extract_features(dataset_images)\n",
    "dump(features, open(\"features.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-telescope",
   "metadata": {},
   "source": [
    "# This process can take a lot of time depending on your system. We are using an Nvidia 1050 GPU for training purpose so it took us around 15 minutes for performing this task. However, if you are using CPU then this process might take 1-2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "substantial-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"features.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-arthritis",
   "metadata": {},
   "source": [
    "#  Loading dataset for Training the model\n",
    "## In our Flickr_8k_test folder, we have Flickr_8k.trainImages.txt file that contains a list of 6000 image names that we will use for training.\n",
    "\n",
    "## For loading the training dataset, we need more functions:\n",
    "\n",
    "## 1.) load_photos( filename ) – This will load the text file in a string and will return the list of image names.\n",
    "## 2.) load_clean_descriptions( filename, photos ) – This function will create a dictionary that contains captions for each photo from the list of photos. We also append the start  and end identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.\n",
    "## 3.) load_features(photos) – This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data \n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    return photos\n",
    "\n",
    "def load_clean_descriptions(filename, photos):   \n",
    "    #loading clean_descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "        \n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "    \n",
    "        image, image_caption = words[0], words[1:]\n",
    "        \n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos):\n",
    "    #loading all features\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "identical-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "\n",
    "#train = loading_data(filename)\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-rolling",
   "metadata": {},
   "source": [
    "# Tokenizing the vocabulary \n",
    "## Computers don’t understand English words, for computers, we will have to represent them with numbers. So, we will map each word of the vocabulary with a unique index value. Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a “tokenizer.p” pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "therapeutic-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting dictionary to clean list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjustable-consent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7577"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give each word a index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-lambda",
   "metadata": {},
   "source": [
    "# Our vocabulary contains 7577 words.\n",
    "## We calculate the maximum length of the descriptions. This is important for deciding the model structure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "french-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate maximum length of descriptions\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "\n",
    "max_length = max_length(descriptions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-flour",
   "metadata": {},
   "source": [
    "# Max_length of description is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "tropical-factory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36452794, 0.12713662, 0.0013574 , ..., 0.221817  , 0.01178991,\n",
       "       0.24176797], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['1000268201_693b08cb0e.jpg'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-juvenile",
   "metadata": {},
   "source": [
    "# Create Data generator\n",
    "## To make this task into a supervised learning task, we have to provide input and output to the model for training. We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6000 images is not possible to hold into memory so we will be using a generator method that will yield batches.\n",
    "## The generator will yield the input and output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-pearl",
   "metadata": {},
   "source": [
    "# For Example \n",
    "## The input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict. \n",
    "## It can be better understood by Fig below\n",
    "![alt text](Example.png \"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "enabling-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input-output sequence pairs from the image description.\n",
    "\n",
    "# data generator, used by model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "            #retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "            yield [[input_image, input_sequence], output_word]         \n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "christian-escape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47, 2048), (47, 32), (47, 7577))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check the shape of the input and output for your model\n",
    "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-constitutional",
   "metadata": {},
   "source": [
    "# Defining the CNN-RNN model\n",
    "## To define the structure of the model, we will be using the Keras Model from Functional API. It will consist of three major parts:\n",
    "## Feature Extractor – The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.\n",
    "## Sequence Processor – An embedding layer will handle the textual input, followed by the LSTM layer.\n",
    "## Decoder – By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-accounting",
   "metadata": {},
   "source": [
    "# Visual representation of the final model can be seen as\n",
    "![alt text](model.png \"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "genuine-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_crossentropy'])\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-philadelphia",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "## To train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on the system capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7577\n",
      "Description Length:  32\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 32, 256)      1939712     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2048)         0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 256)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          524544      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          525312      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_10[0][0]                   \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 256)          65792       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 7577)         1947289     dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,002,649\n",
      "Trainable params: 5,002,649\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From D:\\Miniconda\\envs\\data_science\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "1011/6000 [====>.........................] - ETA: 27:33 - loss: 5.4170 - categorical_crossentropy: 5.4336"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(train_descriptions)\n",
    "# making a directory models to save our models\n",
    "os.mkdir(\"models\")\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "metric-crystal",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7787438e2f36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "print(model.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history['categorical_crossentropy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-snowboard",
   "metadata": {},
   "source": [
    "# After Training the Model, we will have to Test the model based on training to understand how it's performing on different datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
